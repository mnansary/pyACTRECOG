{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "convNet3D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnansary/pyF2O/blob/master/colab_gen_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1ojVYZ7Spzpv"
      },
      "source": [
        "# colab specific task\n",
        "*   mount google drive\n",
        "*   TPU check\n",
        "*   Check TF version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsY7_mMwWC3h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "b6fe4733-6190-44ca-90c3-369f7072a45e"
      },
      "source": [
        "!pip3 install tensorflow==1.13.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.17.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (41.6.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.16.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (3.0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "--q4JaV2ps6z",
        "outputId": "618f2922-8c1a-47a5-8616-1e64bccb8262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9NVN35lELc_p",
        "outputId": "e47b86b6-561e-4b41-99c4-0e72e03edf5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "\n",
        "# tpu check\n",
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.16.101.162:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 7858302473141818004),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 12623185630599305096),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 4937265352828163642),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5445691478193049863),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5494180979706028767),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 10769982321566407128),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8558279051891193923),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15998393232634022417),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 10293437951911252114),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15715635651003287985),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 12856684131251307101)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_HoJM2SopemQ"
      },
      "source": [
        "# GCS specific task \n",
        "* **auth user**\n",
        "* **save** and **upload** credentials to **tpu**\n",
        "* set project information\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cAxBhxEtlLm9",
        "colab": {}
      },
      "source": [
        "# auth user for cloud SDK\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O-JJkYZILgmi",
        "outputId": "e301d949-c67d-4c5c-c332-34acbe0f6a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Save credentials\n",
        "import json\n",
        "SERVICE_KEY_PATH='/content/adc.json' # @param\n",
        "# Upload credentials to TPU.\n",
        "with tf.Session(TPU_ADDRESS) as sess:    \n",
        "    with open(SERVICE_KEY_PATH, 'r') as f:\n",
        "        auth_info = json.load(f)\n",
        "        tf.contrib.cloud.configure_gcs(sess, credentials=auth_info)\n",
        "# set service_account\n",
        "JSON_DATA=json.load(open(SERVICE_KEY_PATH))\n",
        "SERVICE_ACCOUNT=str(JSON_DATA['client_id']).split('.')[0]\n",
        "print('Service Account:',SERVICE_ACCOUNT)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Service Account: 32555940559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E0MQ_8RgO4Qi"
      },
      "source": [
        "#### SET PROJECT INFORMATION "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4pCaaRGjLRmz",
        "outputId": "a99e3408-ce9d-48fd-971f-938683a3c0ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "PROJECT_ID    ='record-1106154'        # @param \n",
        "BUCKET        ='tfalldata'             # @param \n",
        "RECORD_IDEN  = 'TFRECORD'             # @param\n",
        "# LIST FILES\n",
        "TFRECORDS_DIR= 'gs://{}/{}/'.format(BUCKET,RECORD_IDEN)\n",
        "#\n",
        "# change TFRECORDS_DIR specific to structre\n",
        "#\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "!gsutil ls {TFRECORDS_DIR}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "gs://tfalldata/TFRECORD/Eval/\n",
            "gs://tfalldata/TFRECORD/Train/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dxefiHZ4qlHA"
      },
      "source": [
        "# ConvNet3D Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "blwtSzOarVYM"
      },
      "source": [
        "## Data Pipeline\n",
        "* set **FLAGS** and **PARAMS**\n",
        "* For TRAIN and EVAL data size check **info.json** in DataSet Dir\n",
        "* define input functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FRPtSIGWin7u"
      },
      "source": [
        "## FLAGS AND PARAMS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BOro7D1krWYf",
        "outputId": "8cd20bc6-a77b-44ac-bb90-395f42dbc5db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "\n",
        "class FLAGS:\n",
        "    IMAGE_DIM       = 128 # @param\n",
        "    NB_CHANNELS     = 1   # @param\n",
        "    BATCH_SIZE      = 8 # @param\n",
        "    SHUFFLE_BUFFER  = 1000 # @param\n",
        "    NB_CLASSES      = 17   # @param\n",
        "    MIN_SEQ_LEN     = 6    # @param\n",
        "    \n",
        "NB_TRAIN_DATA       = 49920 # @param\n",
        "NB_EVAL_DATA        = 3456  # @param\n",
        "EPOCHS            = 10   # @param\n",
        "NB_TOTAL_DATA= NB_TRAIN_DATA+ NB_EVAL_DATA\n",
        "STEPS_PER_EPOCH     =  NB_TOTAL_DATA // FLAGS.BATCH_SIZE \n",
        "VALIDATION_STEPS    =  NB_EVAL_DATA  // FLAGS.BATCH_SIZE \n",
        "\n",
        "print('Steps Per epoch:',STEPS_PER_EPOCH)\n",
        "print('Validation Steps:', VALIDATION_STEPS)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps Per epoch: 6672\n",
            "Validation Steps: 432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6C3FkEX-iu9n"
      },
      "source": [
        "## Data Input Functions\n",
        "* get **bucket** \n",
        "* define **train_in_fn()** and **eval_in_fn()**\n",
        "* **NOTE:AVOID USING PARTIALS** \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yjBH4t36PydC",
        "outputId": "269a13c5-d867-481a-f3f4-305dd63c4073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.cloud import storage\n",
        "client = storage.Client(PROJECT_ID)\n",
        "# get bucket from the project\n",
        "bucket=client.get_bucket(BUCKET)\n",
        "print(bucket)\n",
        "def data_input_fn(FLAGS,mode): \n",
        "    \n",
        "    def _parser(example):\n",
        "        data  ={ 'feats':tf.io.FixedLenFeature((FLAGS.MIN_SEQ_LEN,FLAGS.IMAGE_DIM,FLAGS.IMAGE_DIM,FLAGS.NB_CHANNELS),tf.int64),\n",
        "                 'label':tf.io.FixedLenFeature((),tf.int64)\n",
        "        }    \n",
        "        parsed_example=tf.io.parse_single_example(example,data)\n",
        "        feats=tf.cast(parsed_example['feats'],tf.float32)/255.0\n",
        "        feats=tf.reshape(feats,(FLAGS.MIN_SEQ_LEN,FLAGS.IMAGE_DIM,FLAGS.IMAGE_DIM,FLAGS.NB_CHANNELS))\n",
        "        \n",
        "        idx = tf.cast(parsed_example['label'], tf.int64)\n",
        "        label=tf.one_hot(idx,FLAGS.NB_CLASSES,dtype=tf.int64)\n",
        "        return feats,label\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset([os.path.join('gs://{}/'.format(BUCKET), f.name) for f in bucket.list_blobs(prefix='{}/{}'.format(RECORD_IDEN,mode))])\n",
        "    dataset = dataset.shuffle(FLAGS.SHUFFLE_BUFFER,reshuffle_each_iteration=True)\n",
        "    dataset = dataset.map(_parser)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(FLAGS.BATCH_SIZE,drop_remainder=True)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    X, y = iterator.get_next()\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "      feats=X.eval()\n",
        "      labels=y.eval()\n",
        "      yield feats,labels\n",
        "\n",
        "def train_in_fn():\n",
        "  return data_input_fn(FLAGS,'Train')\n",
        "  \n",
        "  return data_input_fn(FLAGS,'Train')\n",
        "def eval_in_fn():\n",
        "  return data_input_fn(FLAGS,'Eval')\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Bucket: tfalldata>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xIGSxPbFgwZh"
      },
      "source": [
        "## Change to git repo dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ySSaUDNQgwBK",
        "outputId": "c46a4ca8-d8ae-4b12-f5bc-ce524c95f923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/gdrive/My\\ Drive/PROJECTS/HACT/pyACTRECOG/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/PROJECTS/HACT/pyACTRECOG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XMWNlaqLNIPY"
      },
      "source": [
        "## COMPILE MODEL\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_WMdSlCZJPWi",
        "outputId": "f7643758-345b-486c-ac03-085e64e83c0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append('.')\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from coreLib.model import convNet3D\n",
        "\n",
        "model=convNet3D(seq_len=FLAGS.MIN_SEQ_LEN,\n",
        "                img_dim=FLAGS.IMAGE_DIM,\n",
        "                nb_channels=FLAGS.NB_CHANNELS,\n",
        "                nb_classes=FLAGS.NB_CLASSES)\n",
        "model.summary()\n",
        "model.compile(optimizer=tf.train.RMSPropOptimizer(1e-2), \n",
        "              loss=categorical_crossentropy,\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 6, 128, 128, 1)    0         \n",
            "_________________________________________________________________\n",
            "INITIAL_CONV3D (Conv3D)      (None, 6, 128, 128, 64)   1792      \n",
            "_________________________________________________________________\n",
            "INITIAL_POOL3D (MaxPooling3D (None, 6, 64, 64, 64)     0         \n",
            "_________________________________________________________________\n",
            "CONV3D_1_C1 (Conv3D)         (None, 6, 64, 64, 128)    221312    \n",
            "_________________________________________________________________\n",
            "POOL3D_1 (MaxPooling3D)      (None, 6, 32, 32, 128)    0         \n",
            "_________________________________________________________________\n",
            "CONV3D_2_C1 (Conv3D)         (None, 6, 32, 32, 256)    884992    \n",
            "_________________________________________________________________\n",
            "CONV3D_2_c2 (Conv3D)         (None, 6, 32, 32, 256)    1769728   \n",
            "_________________________________________________________________\n",
            "POOL3D_2 (MaxPooling3D)      (None, 6, 16, 16, 256)    0         \n",
            "_________________________________________________________________\n",
            "CONV3D_3_C1 (Conv3D)         (None, 6, 16, 16, 512)    3539456   \n",
            "_________________________________________________________________\n",
            "CONV3D_3_c2 (Conv3D)         (None, 6, 16, 16, 512)    7078400   \n",
            "_________________________________________________________________\n",
            "POOL3D_3 (MaxPooling3D)      (None, 6, 8, 8, 512)      0         \n",
            "_________________________________________________________________\n",
            "CONV3D_4_C1 (Conv3D)         (None, 6, 8, 8, 512)      7078400   \n",
            "_________________________________________________________________\n",
            "CONV3D_4_c2 (Conv3D)         (None, 6, 8, 8, 512)      7078400   \n",
            "_________________________________________________________________\n",
            "ZERO_PAD_LAST_CONV (ZeroPadd (None, 6, 10, 10, 512)    0         \n",
            "_________________________________________________________________\n",
            "POOL3D_4 (MaxPooling3D)      (None, 6, 5, 5, 512)      0         \n",
            "_________________________________________________________________\n",
            "FLATTEN (Flatten)            (None, 76800)             0         \n",
            "_________________________________________________________________\n",
            "DENSE_1 (Dense)              (None, 4096)              314576896 \n",
            "_________________________________________________________________\n",
            "DROP_1 (Dropout)             (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "DENSE_2 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "DROP_2 (Dropout)             (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "DENSE_CLASS (Dense)          (None, 17)                69649     \n",
            "=================================================================\n",
            "Total params: 359,080,337\n",
            "Trainable params: 359,080,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNNe-VLhb7p9",
        "colab_type": "text"
      },
      "source": [
        "## EXPORT TO TPU MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8XLB7PLb_SO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "604fbb70-40c5-45c3-fc04-ec86b30b7c78"
      },
      "source": [
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "cluster_resolver=tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)\n",
        "strategy=tf.contrib.tpu.TPUDistributionStrategy(cluster_resolver)\n",
        "model = tf.contrib.tpu.keras_to_tpu_model(model,strategy=strategy)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.16.101.162:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 7858302473141818004)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 12623185630599305096)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 4937265352828163642)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5445691478193049863)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5494180979706028767)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 10769982321566407128)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8558279051891193923)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15998393232634022417)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 10293437951911252114)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15715635651003287985)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 12856684131251307101)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9nQXx8YRNSyr"
      },
      "source": [
        "## Train\n",
        "* define **checkpoints** and **callbacks** (tensorboard avoided)\n",
        "* train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OWTUQlweNYYN",
        "outputId": "cef971e1-01dd-4a41-b86b-af1ecb563359",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "MODEL_DIR=os.path.join(os.getcwd(),'info')\n",
        "MODEL_NAME='convNet3D' # @param\n",
        "\n",
        "history =  model.fit_generator(\n",
        "              train_in_fn(),\n",
        "              epochs= EPOCHS,\n",
        "              steps_per_epoch= STEPS_PER_EPOCH,\n",
        "              validation_data=eval_in_fn(),\n",
        "              validation_steps= VALIDATION_STEPS,\n",
        "              verbose=1\n",
        "            )\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(1,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(1, 6, 128, 128, 1), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(1, 17), dtype=tf.float32, name='DENSE_CLASS_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 28.62976050376892 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: From /job:worker/replica:0/task:0:\n8 root error(s) found.\n  (0) Resource exhausted: Attempting to reserve 2.65G at the bottom of memory. That was not possible. There are 1.12G free, 2.34G reserved, and 2.46G reservable.; \nHBM capacity 8.00G:\n       Top-of-Memory reserved:  528.00M (     553,648,128 bytes)\n          Currently allocated:    4.53G (   4,864,262,144 bytes)\n          Program requirement:    2.65G (   2,849,522,688 bytes)\n                               ------------\n      Available after request:  307.56M (     322,501,632 bytes)\n; attempting reservation for program memory\n\nTotal hbm usage >= 7.72G:\n    reserved                 528.00M \n    persistent allocations     4.53G (11.4% fragmentation)\n    program                    2.67G \n\nPersistent allocations include some or all of:\n    arguments                  4.01G (100.0% utilization)\n    output                     4.01G (100.0% utilization) [may share some memory with arguments]\n\nProgram hbm requirement 2.67G:\n    reserved           4.0K\n    global            84.0K\n    scoped            13.0K\n    HLO temp          2.65G (91.6% utilization, 0.0% fragmentation (440.0K))\n    overlays         22.27M\n\n  Largest program allocations in hbm:\n\n  1. Size: 1.17G\n     Operator: op_type=\"CrossReplicaSum\" op_name=\"training/TFOptimizer/CrossReplicaSum_16\"\n     Shape: f32[76800,4096]{1,0:T(8,128)}\n     Unpadded size: 1.17G\n     XLA label: %all-reduce.18 = f32[76800,4096]{1,0:T(8,128)} all-reduce(f32[76800,4096]{1,0:T(8,128)} %fusion), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=%sum.1332, metadata={op_type=\"CrossReplicaSum\" op_name=\"training/TFOptimizer/CrossReplicaSum_16\"}, backend_config...\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 1.17G\n     Operator: op_type=\"MatMul\" op_name=\"training/TFOptimizer/gradients/tpu_139914548113816/DENSE_1/MatMul_grad/MatMul_1\"\n     Shape: f32[76800,4096]{1,0:T(8,128)}\n     Unpadded size: 1.17G\n     XLA label: %fusion = f32[76800,4096]{1,0:T(8,128)} fusion(f32[4096]{0:T(1024)} %fusion.94, f32[76800]{0:T(1024)} %reshape.70), kind=kLoop, calls=%fused_computation, metadata={op_type=\"MatMul\" op_name=\"training/TFOptimizer/gradients/tpu_139914548113816/DENSE_1/MatMul_...\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 96.00M\n     Operator: op_type=\"InfeedDequeueTuple\" op_name=\"infeed-train\"\n     Shape: bf16[1,6,128,128,1]{4,0,3,2,1:T(4,128)(2,1)}\n     Unpadded size: 192.0K\n     Extra memory due to padding: 95.81M (512.0x expansion)\n     XLA label: %copy.39 = bf16[1,6,128,128,1]{4,0,3,2,1:T(4,128)(2,1)} copy(f32[1,6,128,128,1]{3,2,1,4,0:T(8,128)} %get-tuple-element.563), metadata={op_type=\"InfeedDequeueTuple\" op_name=\"infeed-train\"}\n     Allocation type: HLO temp\n     ==========================\n\n  4. Size: 96.00M\n     Operator: op_type=\"Conv3D\" op_name=\"tpu_139914548113816/INITIAL_CONV3D/Conv3D\"\n     Shape: f32[1,6,128,128,64]{4,0,3,2,1:T(2,128)}\n     Unpadded size: 24.00M\n     Extra memory due to padding: 72.00M (4.0x expansion)\n     XLA label: %fusion.10 = f32[1,6,128,128,64]{4,0,3,2,1:T(2,128)} fusion(f32[64]{0:T(256)} %get-tuple-element.693, bf16[1,6,128,128,1]{4,0,3,2,1:T(4,128)(2,1)} %copy.39, f32[3,3,3,1,64]{4,3,2,1,0:T(2,128)} %copy.40), kind=kOutput, calls=%fused_computation.10, metadata=...\n     Allocation type: HLO temp\n     ==========================\n\n  5. Size: 27.00M\n     Shape: f32[3,3,3,512,512]{4,3,2,1,0:T(8,128)}\n     Unpadded size: 27.00M\n     XLA label: %copy.57 = f32[3,3,3,512,512]{4,3,2,1,0:T(8,128)} copy(f32[3,3,3,512,512]{4,3,2,1,0:T(8,128)} %get-tuple-element.682)\n     Allocation type: HLO temp\n     ==========================\n\n  6. Size: 24.00M\n     Operator: op_type=\"MaxPool3D\" op_name=\"tpu_139914548113816/INITIAL_POOL3D/MaxPool3D\"\n     Shape: bf16[1,6,64,64,64]{4,0,3,2,1:T(4,128)(2,1)}\n     Unpadded size: 3.00M\n     Extra memory due to padding: 21.00M (8.0x expansion)\n     XLA label: %reduce-window = bf16[1,6,64,64,64]{4,0,3,2,1:T(4,128)(2,1)} reduce-window(f32[1,6,128,128,64]{4,0,3,2,1:T(2,128)} %fusion.10, bf16[]{:T(512)(2,1)} %constant.707), window={size=1x1x2x2x1 stride=1x1x2x2x1}, to_apply=%max_F32.162, metadata={op_type=\"MaxPool3...\n     Allocation type: HLO temp\n     ==========================\n\n  7. Size: 24.00M\n     Operator: op_type=\"Conv3D\" op_name=\"tpu_139914548113816/CONV3D_1_C1/Conv3D\"\n     Shape: f32[1,6,64,64,128]{4,0,3,2,1:T(2,128)}\n     Unpadded size: 12.00M\n     Extra memory due to padding: 12.00M (2.0x expansion)\n     XLA label: %fusion.36 = f32[1,6,64,64,128]{4,0,3,2,1:T(2,128)} fusion(f32[128]{0:T(256)} %get-tuple-element.673, bf16[1,6,64,64,64]{4,0,3,2,1:T(4,128)(2,1)} %reduce-window, f32[3,3,3,64,128]{4,3,2,1,0:T(8,128)} %get-tuple-element.674), kind=kOutput, calls=%fused_comp...\n     Allocation type: HLO temp\n     ==========================\n\n  8. Size: 22.27M\n     XLA label: overlays\n     Allocation type: overlays\n     ==========================\n\n  9. Size: 12.00M\n     Operator: op_type=\"Conv3D\" op_name=\"tpu_139914548113816/CONV3D_2_C1/Conv3D\"\n     Shape: f32[1,6,32,32,256]{4,0,3,2,1:T(2,128)}\n     Unpadded size: 6.00M\n     Extra memory due to padding: 6.00M (2.0x expansion)\n     XLA label: %fusion.41 = f32[1,6,32,32,256]{4,0,3,2,1:T(2,128)} fusion(f32[256]{0:T(256)} %get-tuple-element.675, bf16[1,6,32,32,128]{4,0,3,2,1:T(4,128)(2,1)} %reduce-window.1, f32[3,3,3,128,256]{4,3,2,1,0:T(8,128)} %get-tuple-element.676), kind=kOutput, calls=%fused_...\n     Allocation type: HLO temp\n     ==========================\n\n  10. Size: 12.00M\n     Operator: op_type=\"Conv3D\" op_name=\"tpu_139914548113816/CONV3D_2_c2/Conv3D\"\n     Shape: f32[1,6,32,32,256]{4,0,3,2,1:T(2,128)}\n     Unpadded size: 6.00M\n     Extra memory due to padding: 6.00M (2.0x expansion)\n     XLA label: %fusion.42 = f32[1,6,32,32,256]{4,0,3,2,1:T(2,128)} fusion(f32[256]{0:T(256)} %get-tuple-element.677, f32[1,6,32,32,256]{4,0,3,2,1:T(2,128)} %fusion.41, f32[3,3,3,256,256]{4,3,2,1,0:T(8,128)} %get-tuple-element.678), kind=kOutput, calls=%fused_computation....\n     Allocation type: HLO temp\n     ==========================\n\n  11. Size: 6.00M\n     Operator: op_type=\"MaxPool3D\" op_name=\"tpu_139914548113816/POOL3D_1/MaxPool3D\"\n     Shape: bf16[1,6,32,32,128]{4,0,3,2,1:T(4,128)(2,1)}\n     Unpadded size: 1.50M\n     Extra memory due to padding: 4.50M (4.0x expansion)\n     XLA label: %reduce-window.1 = bf16[1,6,32,32,128]{4,0,3,2,1:T(4,128)(2,1)} reduce-window(f32[1,6,64,64,128]{4,0,3,2,1:T(2,128)} %fusion.36, bf16[]{:T(512)(2,1)} %constant.707), window={size=1x1x2x2x1 stride=1x1x2x2x1}, to_apply=%max_F32.174, metadata={op_type=\"MaxPoo...\n     Allocation type: HLO temp\n     ==========================\n\n  12. Size: 6.00M\n     Operator: op_type=\"Conv3D\" op_name=\"tpu_139914548113816/CONV3D_3_C1/Conv3D\"\n     Shape: f32[1,6,16,16,512]{4,0,3,2,1:T(2,128)}\n     Unpadded size: 3.00M\n     Extra memory due to padding: 3.00M (2.0x expansion)\n     XLA label: %fusion.62 = f32[1,6,16,16,512]{4,0,3,2,1:T(2,128)} fusion(f32[512]{0:T(512)} %get-tuple-element.679, bf16[1,6,16,16,256]{4,0,3,2,1:T(4,128)(2,1)} %reduce-window.2, f32[3,3,3,256,512]{4,3,2,1,0:T(8,128)} %get-tuple-element.680), kind=kOutput, calls=%fused_...\n     Allocation type: HLO temp\n     ==========================\n\n  13. Size: 6.00M\n     Operator: op_type=\"Conv3D\" op_name=\"tpu_139914548113816/CONV3D_3_c2/Conv3D\"\n     Shape: f32[1,6,16,16,512]{4,0,3,2,1:T(2,128)}\n     Unpadded size: 3.00M\n     Extra memory due to padding: 3.00M (2.0x expansion)\n     XLA label: %fusion.63 = f32[1,6,16,16,512]{4,0,3,2,1:T(2,128)} fusion(f32[512]{0:T(512)} %get-tuple-element.681, f32[1,6,16,16,512]{4,0,3,2,1:T(2,128)} %fusion.62, f32[3,3,3,512,512]{4,3,2,1,0:T(8,128)} %copy.57), kind=kOutput, calls=%fused_computation.62, metadata={...\n     Allocation type: HLO temp\n     ==========================\n\n  14. Size: 3.00M\n     Operator: op_type=\"MaxPool3D\" op_name=\"tpu_139914548113816/POOL3D_2/MaxPool3D\"\n     Shape: bf16[1,6,16,16,256]{4,0,3,2,1:T(4,128)(2,1)}\n     Unpadded size: 768.0K\n     Extra memory due to padding: 2.25M (4.0x expansion)\n     XLA label: %reduce-window.2 = bf16[1,6,16,16,256]{4,0,3,2,1:T(4,128)(2,1)} reduce-window(f32[1,6,32,32,256]{4,0,3,2,1:T(2,128)} %fusion.42, bf16[]{:T(512)(2,1)} %constant.707), window={",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a4f2006815ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_in_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m               \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mVALIDATION_STEPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1189\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfeed_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutfeed_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m     ], infeed_dict)\n\u001b[0m\u001b[1;32m   1270\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfeed_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: From /job:worker/replica:0/task:0:\n8 root error(s) found.\n  (0) Resource exhausted: Attempting to reserve 2.65G at the bottom of memory. That was not possible. There are 1.12G free, 2.34G reserved, and 2.46G reservable.; \nHBM capacity 8.00G:\n       Top-of-Memory reserved:  528.00M (     553,648,128 bytes)\n          Currently allocated:    4.53G (   4,864,262,144 bytes)\n          Program requirement:    2.65G (   2,849,522,688 bytes)\n                               ------------\n      Available after request:  307.56M (     322,501,632 bytes)\n; attempting reservation for program memory\n\nTotal hbm usage >= 7.72G:\n    reserved                 528.00M \n    persistent allocations     4.53G (11.4% fragmentation)\n    program                    2.67G \n\nPersistent allocations include some or all of:\n    arguments                  4.01G (100.0% utilization)\n    output                     4.01G (100.0% utilization) [may share some memory with arguments]\n\nProgram hbm requirement 2.67G:\n    reserved           4.0K\n    global            84.0K\n    scoped            13.0K\n    HLO temp          2.65G (91.6% utilization, 0.0% fragmentation (440.0K))\n    overlays         22.27M\n\n  Largest program allocations in hbm:\n\n  1. Size: 1.17G\n     Operator: op_type=\"CrossReplicaSum\" op_name=\"training/TFOptimizer/CrossReplicaSum_16\"\n     Shape: f32[76800,4096]{1,0:T(8,128)}\n     Unpadded size: 1.17G\n     XLA label: %all-reduce.18 = f32[76800,4096]{1,0:T(8,128)} all-reduce(f32[76800,4096]{1,0:T(8,128)} %fusion), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=%sum.1332, metadata={op_type=\"CrossReplicaSum\" op_name=\"training/TFOptimizer/CrossReplicaSum_16\"}, backend_config...\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 1.17G\n     Operator: op_type=\"MatMul\" op_name=\"training/TFOptimizer/gradients/tpu_139914548113816/DENSE_1/MatMul_grad/MatMul_1\"\n     Shape: f32[76800,4096]{1,0:T(8,128)}\n     Unpadded size: 1.17G\n     XLA label: %fusion = f32[76800,4096]{1,0:T(8,128)} fusion(f32[4096]{0:T(1024)} %fusion.94, f32[76800]{0:T(1024)} %reshape.70), kind=kLoop, calls=%fused_computation, metadata={op_type=\"MatMul\" op_name=\"training/TFOptimizer/gradients/tpu_139914548113816/DENSE_1/MatMul_...\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 96.00M\n     Operator: op_type=\"InfeedDequeueTuple\" op_name=\"infeed-train\"\n     Shape: bf16[1,6,128,128,1]{4,0,3,2,1:T(4,128)(2,1)}\n     Unpadded size: 192.0K\n     Extra memory due to padding: 95.81M (512.0x expansion)\n     XLA label: %copy.39 = bf16[1,6,128,128,1]{4,0,3,2,1:T(4,128)(2,1)} copy(f32[1,6,128,128,1]{3,2,1,4,0:T(8,128)} %get-tuple-element.563), metadata={op_type=\"InfeedDequeueTuple\" op_name=\"infeed-train\"}\n     Allocation type: HLO temp\n     ==========================\n\n  4. Size: 96.00M\n     Operator: op_type=\"Conv3D\" op_name=\"tpu_139914548113816/INITIAL_CONV3D/Conv3D\"\n     Shape: f32[1,6,128,128,64]{4,0,3,2,1:T(2,128)}\n     Unpadded size: 24.00M\n     Extra memory due to padding: 72.00M (4.0x expansion)\n     XLA label: %fusion.10 = f32[1,6,128,128,64]{4,0,3,2,1:T(2,128)} fusion(f32[64]{0:T(256)} %get-tuple-element.693, bf16[1,6,128,128,1]{4,0,3,2,1:T(4,128)(2,1)} %copy.39, f32[3,3,3,1,64]{4,3,2,1,0:T(2,128)} %copy.40), kind=kOutput, calls=%fused_computation.10, metadata=...\n     Allocation type: HLO temp\n     ==========================\n\n  5. Size: 27.00M\n     Shape: f32[3,3,3,512,512]{4,3,2,1,0:T(8,128)}\n     Unpadded size: 27.00M\n     XLA label: %copy.57 = f32[3,3,3,512,512]{4,3,2,1,0:T(8,128)} copy(f32[3,3,3,512,512]{4,3,2,1,0:T(8,128)} %get-tuple-element.682)\n     Allocation type: HLO temp\n     ==========================\n\n  6. Size: 24.00M\n     Operator: op_type=\"MaxPool3D\" op_name=\"tpu_139914548113816/INITIAL_POOL3D/MaxPool3D\"\n     Shape: bf16[1,6,64,64,64]{4,0,3,2,1:T(4,128)(2,1)}\n     Unpadded size: 3.00M\n     Extra memory due to padding: 21.00M (8.0x expansion)\n     XLA label: %reduce-window = bf16[1,6,64,64,64]{4,0,3,2,1:T(4,128)(2,1)} reduce-window(f32[1,6,128,128,64]{4,0,3,2,1:T(2,128)} %fusion.10, bf16[]{:T(512)(2,1)} %constant.707), window={size=1x1x2x2x1 stride=1x1x2x2x1}, to_apply=%max_F32.162, metadata={op_type=\"MaxPool3...\n     Allocation type: HLO temp\n     ==========================\n\n  7. Size: 24.00M\n     Operator: op_type=\"Conv3D\" op_name=\"tpu_139914548113816/CONV3D_1_C1/Conv3D\"\n     Shape: f32[1,6,64,64,128]{4,0,3,2,1:T(2,128)}\n     Unpadded size: 12.00M\n     Extra memory due to padding: 12.00M (2.0x expansion)\n     XLA label: %fusion.36 = f32[1,6,64,64,128]{4,0,3,2,1:T(2,128)} fusion(f32[128]{0:T(256)} %get-tuple-element.673, bf16[1,6,64,64,64]{4,0,3,2,1:T(4,128)(2,1)} %reduce-window, f32[3,3,3,64,128]{4,3,2,1,0:T(8,128)} %get-tuple-element.674), kind=kOutput, calls=%fused_comp...\n     Allocation type: HLO temp\n     ==========================\n\n  8. Size: 22.27M\n     XLA label: overlays\n     Allocation type: overlays\n     ==========================\n\n  9. Size: 12.00M\n     Operator: op_type=\"Conv3D\" op_name=\"tpu_139914548113816/CONV3D_2_C1/Conv3D\"\n     Shape: f32[1,6,32,32,256]{4,0,3,2,1:T(2,128)}\n     Unpadded size: 6.00M\n     Extra memory due to padding: 6.00M (2.0x expansion)\n     XLA label: %fusion.41 = f32[1,6,32,32,256]{4,0,3,2,1:T(2,128)} fusion(f32[256]{0:T(256)} %get-tuple-element.675, bf16[1,6,32,32,128]{4,0,3,2,1:T(4,128)(2,1)} %reduce-window.1, f32[3,3,3,128,256]{4,3,2,1,0:T(8,128)} %get-tuple-element.676), kind=kOutput, calls=%fused_...\n     Allocation type: HLO temp\n     ==========================\n\n  10. Size: 12.00M\n     Operator: op_type=\"Conv3D\" op_name=\"tpu_139914548113816/CONV3D_2_c2/Conv3D\"\n     Shape: f32[1,6,32,32,256]{4,0,3,2,1:T(2,128)}\n     Unpadded size: 6.00M\n     Extra memory due to padding: 6.00M (2.0x expansion)\n     XLA label: %fusion.42 = f32[1,6,32,32,256]{4,0,3,2,1:T(2,128)} fusion(f32[256]{0:T(256)} %get-tuple-element.677, f32[1,6,32,32,256]{4,0,3,2,1:T(2,128)} %fusion.41, f32[3,3,3,256,256]{4,3,2,1,0:T(8,128)} %get-tuple-element.678), kind=kOutput, calls=%fused_computation....\n     Allocation type: HLO temp\n     ==========================\n\n  11. Size: 6.00M\n     Operator: op_type=\"MaxPool3D\" op_name=\"tpu_139914548113816/POOL3D_1/MaxPool3D\"\n     Shape: bf16[1,6,32,32,128]{4,0,3,2,1:T(4,128)(2,1)}\n     Unpadded size: 1.50M\n     Extra memory due to padding: 4.50M (4.0x expansion)\n     XLA label: %reduce-window.1 = bf16[1,6,32,32,128]{4,0,3,2,1:T(4,128)(2,1)} reduce-window(f32[1,6,64,64,128]{4,0,3,2,1:T(2,128)} %fusion.36, bf16[]{:T(512)(2,1)} %constant.707), window={size=1x1x2x2x1 stride=1x1x2x2x1}, to_apply=%max_F32.174, metadata={op_type=\"MaxPoo...\n     Allocation type: HLO temp\n     ==========================\n\n  12. Size: 6.00M\n     Operator: op_type=\"Conv3D\" op_name=\"tpu_139914548113816/CONV3D_3_C1/Conv3D\"\n     Shape: f32[1,6,16,16,512]{4,0,3,2,1:T(2,128)}\n     Unpadded size: 3.00M\n     Extra memory due to padding: 3.00M (2.0x expansion)\n     XLA label: %fusion.62 = f32[1,6,16,16,512]{4,0,3,2,1:T(2,128)} fusion(f32[512]{0:T(512)} %get-tuple-element.679, bf16[1,6,16,16,256]{4,0,3,2,1:T(4,128)(2,1)} %reduce-window.2, f32[3,3,3,256,512]{4,3,2,1,0:T(8,128)} %get-tuple-element.680), kind=kOutput, calls=%fused_...\n     Allocation type: HLO temp\n     ==========================\n\n  13. Size: 6.00M\n     Operator: op_type=\"Conv3D\" op_name=\"tpu_139914548113816/CONV3D_3_c2/Conv3D\"\n     Shape: f32[1,6,16,16,512]{4,0,3,2,1:T(2,128)}\n     Unpadded size: 3.00M\n     Extra memory due to padding: 3.00M (2.0x expansion)\n     XLA label: %fusion.63 = f32[1,6,16,16,512]{4,0,3,2,1:T(2,128)} fusion(f32[512]{0:T(512)} %get-tuple-element.681, f32[1,6,16,16,512]{4,0,3,2,1:T(2,128)} %fusion.62, f32[3,3,3,512,512]{4,3,2,1,0:T(8,128)} %copy.57), kind=kOutput, calls=%fused_computation.62, metadata={...\n     Allocation type: HLO temp\n     ==========================\n\n  14. Size: 3.00M\n     Operator: op_type=\"MaxPool3D\" op_name=\"tpu_139914548113816/POOL3D_2/MaxPool3D\"\n     Shape: bf16[1,6,16,16,256]{4,0,3,2,1:T(4,128)(2,1)}\n     Unpadded size: 768.0K\n     Extra memory due to padding: 2.25M (4.0x expansion)\n     XLA label: %reduce-window.2 = bf16[1,6,16,16,256]{4,0,3,2,1:T(4,128)(2,1)} reduce-window(f32[1,6,32,32,256]{4,0,3,2,1:T(2,128)} %fusion.42, bf16[]{:T(512)(2,1)} %constant.707), window={"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or0XpoYadSF7",
        "colab_type": "text"
      },
      "source": [
        "## Save Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFDwgbUVdSwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(os.path.join(MODEL_DIR,'{}.h5'.format(MODEL_NAME)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J1M9sjHIbSnf"
      },
      "source": [
        "#### Plot Training Histoty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ndNSS7XIbXaK",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('LOSS History')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.savefig(os.path.join(MODEL_DIR,'{}_history.png'.format(model_name)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}