{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "convNet3D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnansary/pyF2O/blob/master/colab_gen_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1ojVYZ7Spzpv"
      },
      "source": [
        "# colab specific task\n",
        "*   mount google drive\n",
        "*   TPU check\n",
        "*   Check TF version\n",
        "*   Change to git repo "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsY7_mMwWC3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "--q4JaV2ps6z",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9NVN35lELc_p",
        "colab": {}
      },
      "source": [
        "\n",
        "# tpu check\n",
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ySSaUDNQgwBK",
        "colab": {}
      },
      "source": [
        "cd /content/gdrive/My\\ Drive/PROJECTS/HACT/pyACTRECOG/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gqGOIRmujw5",
        "colab_type": "text"
      },
      "source": [
        "# GCS specific task \n",
        "* **auth user**\n",
        "* **save** and **upload** credentials to **tpu**\n",
        "* set project information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zBp1CZeupWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# auth user for cloud SDK\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbLxepOqurpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save credentials\n",
        "import json\n",
        "SERVICE_KEY_PATH='/content/adc.json' # @param\n",
        "# Upload credentials to TPU.\n",
        "with tf.Session(TPU_ADDRESS) as sess:    \n",
        "    with open(SERVICE_KEY_PATH, 'r') as f:\n",
        "        auth_info = json.load(f)\n",
        "        tf.contrib.cloud.configure_gcs(sess, credentials=auth_info)\n",
        "# set service_account\n",
        "JSON_DATA=json.load(open(SERVICE_KEY_PATH))\n",
        "SERVICE_ACCOUNT=str(JSON_DATA['client_id']).split('.')[0]\n",
        "print('Service Account:',SERVICE_ACCOUNT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUUry52GuuzY",
        "colab_type": "text"
      },
      "source": [
        "#### SET PROJECT INFORMATION "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LQRPfsCuxl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PROJECT_ID    ='record-1106154'     # @param \n",
        "BUCKET        ='tfalldata'          # @param \n",
        "TFIDEN        ='TFRECORD'            # @param\n",
        "# LIST FILES\n",
        "TFRECORDS_DIR= 'gs://{}/{}/'.format(BUCKET,TFIDEN)\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "!gsutil ls {TFRECORDS_DIR}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dxefiHZ4qlHA"
      },
      "source": [
        "# ConvNet3D Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "blwtSzOarVYM"
      },
      "source": [
        "#### Data \n",
        "* Set **FLAGS** and **PARAMS**\n",
        "* Create **Train** and **Eval** Data Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BOro7D1krWYf",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('.')\n",
        "import numpy as np \n",
        "\n",
        "class FLAGS:\n",
        "    BATCH_SIZE      = 32  #@param\n",
        "    IMAGE_DIM       = 64   #@param\n",
        "    NB_CHANNELS     = 3    #@param\n",
        "    MIN_SEQ_LEN     = 6    #@param\n",
        "    NB_CLASSES      = 17   #@param\n",
        "    SHUFFLE_BUFFER  = 6400 #@param\n",
        "\n",
        "MODEL_DIR           = '/content/gdrive/My Drive/PROJECTS/HACT/Model/' # @param\n",
        "MODEL_NAME          = 'convNet3D' # @param\n",
        "EPOCHS              =  250           # @param\n",
        "NB_TRAIN_DATA       =  49920       # @param\n",
        "NB_EVAL_DATA        =  3456        # @param\n",
        "NB_TOTAL_DATA       =  NB_TRAIN_DATA + NB_EVAL_DATA \n",
        "STEPS_PER_EPOCH     =  NB_TOTAL_DATA // FLAGS.BATCH_SIZE \n",
        "VALIDATION_STEPS    =  NB_EVAL_DATA  // FLAGS.BATCH_SIZE \n",
        "CHECK_DATA          =  False\n",
        "LEARNING_RATE       = 1e-5 #@param\n",
        "\n",
        "LOAD_WEIGHTS=False #@param\n",
        "EPOCHS_DONE_BEFORE_RECONNECT=0  #@param\n",
        "EPOCHS=EPOCHS-EPOCHS_DONE_BEFORE_RECONNECT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1guqi_rau5kA",
        "colab_type": "text"
      },
      "source": [
        "#### Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQy7N0hEu8D5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.cloud import storage\n",
        "from functools import partial\n",
        "\n",
        "client = storage.Client(PROJECT_ID)\n",
        "# get bucket from the project\n",
        "bucket=client.get_bucket(BUCKET)\n",
        "print(bucket)\n",
        "\n",
        "def data_input_fn(FLAGS,mode): \n",
        "    \n",
        "    def _parser(example):\n",
        "      data  ={ 'feats':tf.io.FixedLenFeature((FLAGS.MIN_SEQ_LEN,FLAGS.IMAGE_DIM,FLAGS.IMAGE_DIM,FLAGS.NB_CHANNELS),tf.float32),\n",
        "                'label':tf.io.FixedLenFeature((),tf.int64)\n",
        "      }    \n",
        "      \n",
        "      parsed_example=tf.io.parse_single_example(example,data)\n",
        "      \n",
        "      feats=tf.cast(parsed_example['feats'],tf.float32)\n",
        "      feats=tf.reshape(feats,(FLAGS.MIN_SEQ_LEN,FLAGS.IMAGE_DIM,FLAGS.IMAGE_DIM,FLAGS.NB_CHANNELS))\n",
        "      \n",
        "      idx = tf.cast(parsed_example['label'], tf.int64)\n",
        "      label=tf.one_hot(idx,FLAGS.NB_CLASSES,dtype=tf.int64)\n",
        "      \n",
        "      return feats,label\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset([os.path.join('gs://{}/'.format(BUCKET), f.name) for f in bucket.list_blobs(prefix='{}/{}'.format(TFIDEN,mode))])\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.map(_parser)\n",
        "    dataset = dataset.shuffle(FLAGS.SHUFFLE_BUFFER,reshuffle_each_iteration=True)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(FLAGS.BATCH_SIZE,drop_remainder=True)\n",
        "    dataset = dataset.prefetch(-1) # autotune    \n",
        "    return dataset\n",
        "\n",
        "def train_in_fn():\n",
        "    return data_input_fn(FLAGS,'Train')    \n",
        "\n",
        "def eval_in_fn():    \n",
        "    return data_input_fn(FLAGS,'Eval')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XMWNlaqLNIPY"
      },
      "source": [
        "#### COMPILE MODEL\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_WMdSlCZJPWi",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('.')\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from coreLib.model import convNet3D\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  model=convNet3D(seq_len=FLAGS.MIN_SEQ_LEN,\n",
        "                  img_dim=FLAGS.IMAGE_DIM,\n",
        "                  nb_channels=FLAGS.NB_CHANNELS,\n",
        "                  nb_classes=FLAGS.NB_CLASSES)\n",
        "  model.summary()\n",
        "  model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),   \n",
        "                loss=categorical_crossentropy,\n",
        "                metrics=['accuracy'])\n",
        "  if LOAD_WEIGHTS:\n",
        "    model.load_weights=os.path.join(MODEL_DIR,'{}.h5'.format(MODEL_NAME))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9nQXx8YRNSyr"
      },
      "source": [
        "#### Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OWTUQlweNYYN",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(filepath=os.path.join(MODEL_DIR,'{}.h5'.format(MODEL_NAME)), verbose=1, save_best_only=True)\n",
        "history=model.fit(train_in_fn(),\n",
        "                    epochs= EPOCHS,\n",
        "                    steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                    validation_data=eval_in_fn(),\n",
        "                    validation_steps=VALIDATION_STEPS,\n",
        "                    callbacks=[checkpoint],\n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or0XpoYadSF7",
        "colab_type": "text"
      },
      "source": [
        "#### Save Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFDwgbUVdSwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(os.path.join(MODEL_DIR,'{}_final.h5'.format(MODEL_NAME)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J1M9sjHIbSnf"
      },
      "source": [
        "#### Plot Training Histoty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ndNSS7XIbXaK",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('LOSS History')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.savefig(os.path.join(MODEL_DIR,'{}_history.png'.format(MODEL_NAME)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}