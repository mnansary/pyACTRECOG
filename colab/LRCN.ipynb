{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mnansary/pyF2O/blob/master/colab_gen_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ojVYZ7Spzpv"
   },
   "source": [
    "# colab specific task\n",
    "*   mount google drive\n",
    "*   TPU check\n",
    "*   Check TF version\n",
    "*   Change to git repo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--q4JaV2ps6z"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NVN35lELc_p"
   },
   "outputs": [],
   "source": [
    "\n",
    "# tpu check\n",
    "import os\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "if 'COLAB_TPU_ADDR' not in os.environ:\n",
    "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
    "else:\n",
    "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "  print ('TPU address is', TPU_ADDRESS)\n",
    "\n",
    "  with tf.Session(TPU_ADDRESS) as session:\n",
    "    devices = session.list_devices()\n",
    "    \n",
    "  print('TPU devices:')\n",
    "  pprint.pprint(devices)\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySSaUDNQgwBK"
   },
   "outputs": [],
   "source": [
    "cd /content/gdrive/My\\ Drive/PROJECTS/HACT/pyACTRECOG/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gqGOIRmujw5"
   },
   "source": [
    "# GCS specific task \n",
    "* **auth user**\n",
    "* **save** and **upload** credentials to **tpu**\n",
    "* set project information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zBp1CZeupWY"
   },
   "outputs": [],
   "source": [
    "# auth user for cloud SDK\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vbLxepOqurpI"
   },
   "outputs": [],
   "source": [
    "# Save credentials\n",
    "import json\n",
    "SERVICE_KEY_PATH='/content/adc.json' # @param\n",
    "# Upload credentials to TPU.\n",
    "with tf.Session(TPU_ADDRESS) as sess:    \n",
    "    with open(SERVICE_KEY_PATH, 'r') as f:\n",
    "        auth_info = json.load(f)\n",
    "        tf.contrib.cloud.configure_gcs(sess, credentials=auth_info)\n",
    "# set service_account\n",
    "JSON_DATA=json.load(open(SERVICE_KEY_PATH))\n",
    "SERVICE_ACCOUNT=str(JSON_DATA['client_id']).split('.')[0]\n",
    "print('Service Account:',SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUUry52GuuzY"
   },
   "source": [
    "#### SET PROJECT INFORMATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0LQRPfsCuxl-"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID    ='record-1106154'     # @param \n",
    "BUCKET        ='tfalldata'          # @param \n",
    "TFIDEN        ='TFRECORD'            # @param\n",
    "# LIST FILES\n",
    "TFRECORDS_DIR= 'gs://{}/{}/'.format(BUCKET,TFIDEN)\n",
    "!gcloud config set project {PROJECT_ID}\n",
    "!gsutil ls {TFRECORDS_DIR}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dxefiHZ4qlHA"
   },
   "source": [
    "# ConvNet3D Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "blwtSzOarVYM"
   },
   "source": [
    "#### Data \n",
    "* Set **FLAGS** and **PARAMS**\n",
    "* Create **Train** and **Eval** Data Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "BOro7D1krWYf"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "import numpy as np \n",
    "\n",
    "class FLAGS:\n",
    "    BATCH_SIZE      = 32  #@param\n",
    "    IMAGE_DIM       = 64   #@param\n",
    "    NB_CHANNELS     = 3    #@param\n",
    "    MIN_SEQ_LEN     = 6    #@param\n",
    "    NB_CLASSES      = 17   #@param\n",
    "    SHUFFLE_BUFFER  = 6400 #@param\n",
    "\n",
    "MODEL_DIR           = '/content/gdrive/My Drive/PROJECTS/HACT/Model/' # @param\n",
    "MODEL_NAME          = 'convNet3D' # @param\n",
    "EPOCHS              =  250           # @param\n",
    "NB_TRAIN_DATA       =  49920       # @param\n",
    "NB_EVAL_DATA        =  3456        # @param\n",
    "NB_TOTAL_DATA       =  NB_TRAIN_DATA + NB_EVAL_DATA \n",
    "STEPS_PER_EPOCH     =  NB_TOTAL_DATA // FLAGS.BATCH_SIZE \n",
    "VALIDATION_STEPS    =  NB_EVAL_DATA  // FLAGS.BATCH_SIZE \n",
    "CHECK_DATA          =  False\n",
    "LEARNING_RATE       = 1e-5 #@param\n",
    "\n",
    "LOAD_WEIGHTS=False #@param\n",
    "EPOCHS_DONE_BEFORE_RECONNECT=0  #@param\n",
    "EPOCHS=EPOCHS-EPOCHS_DONE_BEFORE_RECONNECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1guqi_rau5kA"
   },
   "source": [
    "#### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQy7N0hEu8D5"
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from functools import partial\n",
    "\n",
    "client = storage.Client(PROJECT_ID)\n",
    "# get bucket from the project\n",
    "bucket=client.get_bucket(BUCKET)\n",
    "print(bucket)\n",
    "\n",
    "def data_input_fn(FLAGS,mode): \n",
    "    \n",
    "    def _parser(example):\n",
    "      data  ={ 'feats':tf.io.FixedLenFeature((FLAGS.MIN_SEQ_LEN,FLAGS.IMAGE_DIM,FLAGS.IMAGE_DIM,FLAGS.NB_CHANNELS),tf.float32),\n",
    "                'label':tf.io.FixedLenFeature((),tf.int64)\n",
    "      }    \n",
    "      \n",
    "      parsed_example=tf.io.parse_single_example(example,data)\n",
    "      \n",
    "      feats=tf.cast(parsed_example['feats'],tf.float32)\n",
    "      feats=tf.reshape(feats,(FLAGS.MIN_SEQ_LEN,FLAGS.IMAGE_DIM,FLAGS.IMAGE_DIM,FLAGS.NB_CHANNELS))\n",
    "      \n",
    "      idx = tf.cast(parsed_example['label'], tf.int64)\n",
    "      label=tf.one_hot(idx,FLAGS.NB_CLASSES,dtype=tf.int64)\n",
    "      \n",
    "      return feats,label\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset([os.path.join('gs://{}/'.format(BUCKET), f.name) for f in bucket.list_blobs(prefix='{}/{}'.format(TFIDEN,mode))])\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.map(_parser)\n",
    "    dataset = dataset.shuffle(FLAGS.SHUFFLE_BUFFER,reshuffle_each_iteration=True)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(FLAGS.BATCH_SIZE,drop_remainder=True)\n",
    "    dataset = dataset.prefetch(-1) # autotune    \n",
    "    return dataset\n",
    "\n",
    "def train_in_fn():\n",
    "    return data_input_fn(FLAGS,'Train')    \n",
    "\n",
    "def eval_in_fn():    \n",
    "    return data_input_fn(FLAGS,'Eval')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XMWNlaqLNIPY"
   },
   "source": [
    "#### COMPILE MODEL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WMdSlCZJPWi"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from coreLib.model import LRCN\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "resolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.contrib.distribute.initialize_tpu_system(resolver)\n",
    "strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
    "with strategy.scope():\n",
    "  model=LRCN(seq_len=FLAGS.MIN_SEQ_LEN,\n",
    "              img_dim=FLAGS.IMAGE_DIM,\n",
    "              nb_channels=FLAGS.NB_CHANNELS,\n",
    "              nb_classes=FLAGS.NB_CLASSES)\n",
    "  model.summary()\n",
    "  model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),   \n",
    "                loss=categorical_crossentropy,\n",
    "                metrics=['accuracy'])\n",
    "  if LOAD_WEIGHTS:\n",
    "    model.load_weights=os.path.join(MODEL_DIR,'{}.h5'.format(MODEL_NAME))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9nQXx8YRNSyr"
   },
   "source": [
    "#### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OWTUQlweNYYN"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(filepath=os.path.join(MODEL_DIR,'{}.h5'.format(MODEL_NAME)), verbose=1, save_best_only=True)\n",
    "history=model.fit(train_in_fn(),\n",
    "                    epochs= EPOCHS,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                    validation_data=eval_in_fn(),\n",
    "                    validation_steps=VALIDATION_STEPS,\n",
    "                    callbacks=[checkpoint],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Or0XpoYadSF7"
   },
   "source": [
    "#### Save Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dFDwgbUVdSwY"
   },
   "outputs": [],
   "source": [
    "model.save_weights(os.path.join(MODEL_DIR,'{}_final.h5'.format(MODEL_NAME)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J1M9sjHIbSnf"
   },
   "source": [
    "#### Plot Training Histoty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndNSS7XIbXaK"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('LOSS History')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.savefig(os.path.join(MODEL_DIR,'{}_history.png'.format(MODEL_NAME)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "convNet3D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
